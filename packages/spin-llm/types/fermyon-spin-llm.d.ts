declare module 'fermyon:spin/llm@2.0.0' {
  /**
   * Perform inferencing using the provided model and prompt with the given optional params
   */
  export function infer(
    model: InferencingModel,
    prompt: string,
    params: InferencingParams | undefined,
  ): InferencingResult;
  /**
   * Generate embeddings for the supplied list of text
   */
  export function generateEmbeddings(
    model: EmbeddingModel,
    text: Array<string>,
  ): EmbeddingsResult;
  /**
   * A Large Language Model.
   */
  export type InferencingModel = string;
  /**
   * Inference request parameters
   */
  export interface InferencingParams {
    /**
     * The maximum tokens that should be inferred.
     *
     * Note: the backing implementation may return less tokens.
     */
    maxTokens: number;
    /**
     * The amount the model should avoid repeating tokens.
     */
    repeatPenalty: number;
    /**
     * The number of tokens the model should apply the repeat penalty to.
     */
    repeatPenaltyLastNTokenCount: number;
    /**
     * The randomness with which the next token is selected.
     */
    temperature: number;
    /**
     * The number of possible next tokens the model will choose from.
     */
    topK: number;
    /**
     * The probability total of next tokens the model will choose from.
     */
    topP: number;
  }
  /**
   * The set of errors which may be raised by functions in this interface
   */
  export type Error =
    | ErrorModelNotSupported
    | ErrorRuntimeError
    | ErrorInvalidInput;
  export interface ErrorModelNotSupported {
    tag: 'model-not-supported';
  }
  export interface ErrorRuntimeError {
    tag: 'runtime-error';
    val: string;
  }
  export interface ErrorInvalidInput {
    tag: 'invalid-input';
    val: string;
  }
  /**
   * Usage information related to the inferencing result
   */
  export interface InferencingUsage {
    /**
     * Number of tokens in the prompt
     */
    promptTokenCount: number;
    /**
     * Number of tokens generated by the inferencing operation
     */
    generatedTokenCount: number;
  }
  /**
   * An inferencing result
   */
  export interface InferencingResult {
    /**
     * The text generated by the model
     * TODO: this should be a stream
     */
    text: string;
    /**
     * Usage information about the inferencing request
     */
    usage: InferencingUsage;
  }
  /**
   * The model used for generating embeddings
   */
  export type EmbeddingModel = string;
  /**
   * Usage related to an embeddings generation request
   */
  export interface EmbeddingsUsage {
    /**
     * Number of tokens in the prompt
     */
    promptTokenCount: number;
  }
  /**
   * Result of generating embeddings
   */
  export interface EmbeddingsResult {
    /**
     * The embeddings generated by the request
     */
    embeddings: Array<Float32Array>;
    /**
     * Usage related to the embeddings generation request
     */
    usage: EmbeddingsUsage;
  }
}
